<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sule Bai | 白苏乐</title>

  <meta name="author" content="Sule Bai">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/cat.jpg">
</head>

<body>
  <table
    style="width:100%;max-width:820px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Sule Bai | 白苏乐</name>
                  </p>
                  <p>Hi! I'm a second-year master student in <a
                      href="https://www.sigs.tsinghua.edu.cn/">Tsinghua University Shenzhen International
                      Graduate School</a>, supervised by Prof. <a href="https://andytang15.github.io/">Yansong Tang</a>.
                    Before that, I received my
                    bachelor's degree from the <a href="https://scs.bupt.edu.cn/">School of Computer Science and
                      Technology</a> in the <a href="https://www.bupt.edu.cn/">Beijing University of Posts
                      and Telecommunications</a> in 2023.</p>
                  <p> My currect research interest lies in Multimodal LLM and its applications.</p>
                  <p style="text-align:center">
                    <a href="bsl23@mails.tsinghua.edu.cn">Email</a> &nbsp;/&nbsp;
                    <a href="https://github.com/SuleBai">Github</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=7M_to8EAAAAJ&hl=zh-CN&oi=ao">Scholar</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:30%;max-width:30%">
                  <img style="width:80%;max-width:80%" alt="profile photo" src="./images/baisule.jpg">
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading>
                  <p>
                    * indicates equal contribution
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/univg-r1.jpg" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle><a href="https://arxiv.org/abs/2505.14231">UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning</a></papertitle>
                  <br>
                  <strong>Sule Bai</strong>, Mingxing Li, Yong Liu, Jing Tang, Haoji Zhang, Lei Sun, Xiangxiang Chu, Yansong Tang
                  <br>
                  <em>arXiv Preprint, 2025</em>
                  <br>
                  <a href="https://arxiv.org/abs/2505.14231">[Paper]</a>
                  <a href="https://github.com/AMAP-ML/UniVG-R1">[Code]</a>
                  <a href="https://amap-ml.github.io/UniVG-R1-page/">[Project Page]</a>
                  <a class="more-link" href="https://github.com/AMAP-ML/UniVG-R1" target="_blank">
                    <img
                      alt="GitHub stars" align="right"
                      src="https://img.shields.io/github/stars/AMAP-ML/UniVG-R1?style=social"
                      style="vertical-align: middle; margin-left: 5px;" />
                  </a>
                  <br>
                  <p>We propose UniVG-R1, a reasoning-guided MLLM for universal visual grounding, which leverages reinforcement learning to enhance reasoning across complex multi-image and multi-modal scenarios.</p>
                </td>
              </tr>
              <p></p>
              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <!-- <img style="width:100%;max-width:100%" src="images/scclip.png" alt="dise"> -->
                  <img style="width:auto; max-width:100%; max-height:120px;" src="images/scclip.png" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle><a href="https://arxiv.org/abs/2411.15869">Self-Calibrated CLIP for Training-Free Open-Vocabulary Segmentation</a></papertitle>
                  <br>
                  <strong>Sule Bai*</strong>, Yong Liu*, Yifei Han, Haoji Zhang, Yansong Tang
                  <br>
                  <em>arXiv Preprint, 2024</em>
                  <br>
                  <a href="https://arxiv.org/abs/2411.15869">[Paper]</a>
                  <a href="https://github.com/SuleBai/SC-CLIP">[Code]</a>
                  <a class="more-link" href="https://github.com/SuleBai/SC-CLIP" target="_blank">
                    <img
                      alt="GitHub stars" align="right"
                      src="https://img.shields.io/github/stars/SuleBai/SC-CLIP?style=social"
                      style="vertical-align: middle; margin-left: 5px;" />
                  </a>
                  <br>
                  <p>We propose a training-free method that enhances CLIP's dense representation through self-calibration without introducing new parameters or relying on additional backbones. </p>
                </td>
              </tr>
              <p></p>
              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/scan.png" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle><a href="https://arxiv.org/abs/2312.04089">Open-Vocabulary Segmentation with Semantic-Assisted Calibration</a></papertitle>
                  <br>
                  Yong Liu*, <strong>Sule Bai*</strong>, Guanbin Li, Yitong Wang, Yansong Tang
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024</em>
                  <br>
                  <a href="https://arxiv.org/abs/2312.04089">[Paper]</a>
                  <a href="https://github.com/workforai/SCAN">[Code]</a>
                  <a class="more-link" href="https://github.com/yongliu20/SCAN" target="_blank">
                    <img
                      alt="GitHub stars" align="right"
                      src="https://img.shields.io/github/stars/yongliu20/SCAN?style=social"
                      style="vertical-align: middle; margin-left: 5px;" />
                  </a>
                  <br>
                  <p> We propose an open-vocabulary segmentation (OVS) method by calibrating in-vocabulary and
                    domain-biased embedding space with generalized contextual prior of CLIP. </p>
                </td>
              </tr>
              <p></p>
              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/nae.jpg" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle><a href="https://arxiv.org/abs/2404.14471">Narrative Action Evaluation with Prompt-Guided Multimodal Interaction</a></papertitle>
                  <br>
                  Shiyi Zhang*, <strong>Sule Bai*</strong>, Guangyi Chen, Lei Chen, Jiwen Lu, Junle Wang, Yansong Tang
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024</em>
                  <br>
                  <a href="https://arxiv.org/abs/2404.14471">[Paper]</a>
                  <a href="https://github.com/shiyi-zh0408/NAE_CVPR2024">[Code]</a>
                  <a class="more-link" href="https://github.com/shiyi-zh0408/NAE_CVPR2024" target="_blank">
                    <img
                      alt="GitHub stars" align="right"
                      src="https://img.shields.io/github/stars/shiyi-zh0408/NAE_CVPR2024?style=social"
                      style="vertical-align: middle; margin-left: 5px;" />
                  </a>
                  <br>
                  <p> We investigate a new problem called narrative action evaluation (NAE) and propose a prompt-guided
                    multimodal interaction framework. </p>
                </td>
              </tr>
              <p></p>
              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/openbench.png" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle><a href="https://arxiv.org/abs/2506.16058">Stepping Out of Similar Semantic Space for Open-Vocabulary Segmentation</a></papertitle>
                  <br>
                  Yong Liu*, Songli Wu*, <strong>Sule Bai*</strong>, Jiahao Wang, Yansong Tang
                  <br>
                  <em>IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>), 2023</em>
                  <br>
                  <a href="https://arxiv.org/abs/2506.16058">[Paper]</a>
                  <a href="https://arxiv.org/abs/2506.16058">[Code]</a>
                  <br>
                  <p> We present a new benchmark that differs significantly from the training semantics. And we propose OVSNet to improve the segmentation performance for diverse and open scenarios.</p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
  <p>
    <center>
      &copy; Sule Bai | Last updated: June 2025
    </center>
  </p>
</body>

</html>