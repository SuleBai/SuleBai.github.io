<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sule Bai</title>

  <meta name="author" content="Sule Bai">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table
    style="width:100%;max-width:820px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Sule Bai</name>
                  </p>
                  <p>Hi! I'm currently a first-year master student in <a href="https://www.sigs.tsinghua.edu.cn/">Tsinghua University Shenzhen International
                    Graduate School</a>, supervised by Prof. <a href="https://andytang15.github.io/">Yansong Tang</a>. Before that, I received my
                    bachelor's degree from the <a href="https://scs.bupt.edu.cn/">School of Computer Science and Technology</a> in the <a href="https://www.bupt.edu.cn/">Beijing University of Posts
                      and Telecommunications</a> in Jun. 2023.</p>
                  <p> My currect research interest lies in open-vocabulary segmentation and video understanding.</p>
                  <p style="text-align:center">
                    <a href="bsl23@mails.tsinghua.edu.cn">Email</a> &nbsp;/&nbsp;
                    <a href="https://github.com/Mythszj">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:30%;max-width:30%">
                  <img style="width:80%;max-width:80%" alt="profile photo" src="./images/baisule.jpg">
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/openseg.png" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle>Open-Vocabulary Segmentation with Semantic-Assisted Calibration</papertitle>
                  <br>
                  Yong Liu*, <strong>Sule Bai*</strong>, Guanbin Li, Yitong Wang, Yansong Tang
                  <br>
                  *equal contribution
                  <br>
                  <em>CVPR 2024</em>
                  <br>
                  <a href="https://arxiv.org/pdf/2312.04089.pdf">[Paper]</a>
                  <a href="https://github.com/workforai/SCAN">[Code]</a>
                  <br>
                  <p> We propose an open-vocabulary segmentation (OVS) method by calibrating in-vocabulary and
                    domain-biased embedding space with generalized contextual prior of CLIP. </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/aqa-mi.jpg" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle>Narrative Action Evaluation with Prompt-Guided Multimodal Interaction</papertitle>
                  <br>
                  Shiyi Zhang*, <strong>Sule Bai*</strong>, Guangyi Chen, Lei Chen, Jiwen Lu, Junle Wang, Yansong Tang
                  <br>
                  *equal contribution
                  <br>
                  <em>CVPR 2024</em>
                  <br>
                  <a>[Coming Soon]</a>
                  <br>
                  <p> We investigate a new problem called narrative action evaluation (NAE) and propose a prompt-guided
                    multimodal interaction framework. </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    <a href="https://jonbarron.info/">Thanks to Jon Barron.</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- <a href="https://clustrmaps.com/site/1bevc" title="Visit tracker"><img
              src="//www.clustrmaps.com/map_v2.png?d=pSqGxdyauRrVjsHq2YSTApeSzMqOewuzLgOt0g6Ow6Y&cl=ffffff" /></a> -->

        </td>
      </tr>
  </table>
</body>

</html>