<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sule Bai | 白苏乐</title>

  <meta name="author" content="Sule Bai">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/cat.jpg">
</head>

<body>
  <table
    style="width:100%;max-width:820px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Sule Bai | 白苏乐</name>
                  </p>
                  <p>Hi! I'm a second-year master student in <a
                      href="https://www.sigs.tsinghua.edu.cn/">Tsinghua University Shenzhen International
                      Graduate School</a>, supervised by Prof. <a href="https://andytang15.github.io/">Yansong Tang</a>.
                    Before that, I received my
                    bachelor's degree from the <a href="https://scs.bupt.edu.cn/">School of Computer Science and
                      Technology</a> in the <a href="https://www.bupt.edu.cn/">Beijing University of Posts
                      and Telecommunications</a> in 2023.</p>
                  <p> My currect research interest lies in open-vocabulary segmentation, vision-language models and video understanding.</p>
                  <p style="text-align:center">
                    <a href="bsl23@mails.tsinghua.edu.cn">Email</a> &nbsp;/&nbsp;
                    <a href="https://github.com/SuleBai">Github</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=7M_to8EAAAAJ&hl=zh-CN&oi=ao">Scholar</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:30%;max-width:30%">
                  <img style="width:80%;max-width:80%" alt="profile photo" src="./images/baisule.jpg">
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading>
                  <p>
                    * indicates equal contribution
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/scclip.png" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle><a href="https://arxiv.org/abs/2411.15869">Self-Calibrated CLIP for Training-Free Open-Vocabulary Segmentation</a></papertitle>
                  <br>
                  <strong>Sule Bai*</strong>, Yong Liu*, Yifei Han, Haoji Zhang, Yansong Tang
                  <br>
                  <em>arXiv Preprint</em>
                  <br>
                  <a href="https://arxiv.org/abs/2411.15869">[Paper]</a>
                  <a href="https://github.com/SuleBai/SC-CLIP">[Code]</a>
                  <br>
                  <p>We propose a training-free method that enhances CLIP's segmentation performance through self-calibration without introducing new parameters or relying on additional backbones. </p>
                </td>
              </tr>
              <p></p>
              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/openseg.png" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle><a href="https://arxiv.org/abs/2312.04089">Open-Vocabulary Segmentation with Semantic-Assisted Calibration</a></papertitle>
                  <br>
                  Yong Liu*, <strong>Sule Bai*</strong>, Guanbin Li, Yitong Wang, Yansong Tang
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024</em>
                  <br>
                  <a href="https://arxiv.org/abs/2312.04089">[Paper]</a>
                  <a href="https://github.com/workforai/SCAN">[Code]</a>
                  <br>
                  <p> We propose an open-vocabulary segmentation (OVS) method by calibrating in-vocabulary and
                    domain-biased embedding space with generalized contextual prior of CLIP. </p>
                </td>
              </tr>
              <p></p>
              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/aqa-mi.jpg" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle><a href="https://arxiv.org/abs/2404.14471">Narrative Action Evaluation with Prompt-Guided Multimodal Interaction</a></papertitle>
                  <br>
                  Shiyi Zhang*, <strong>Sule Bai*</strong>, Guangyi Chen, Lei Chen, Jiwen Lu, Junle Wang, Yansong Tang
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024</em>
                  <br>
                  <a href="https://arxiv.org/abs/2404.14471">[Paper]</a>
                  <a href="https://github.com/shiyi-zh0408/NAE_CVPR2024">[Code]</a>
                  <br>
                  <p> We investigate a new problem called narrative action evaluation (NAE) and propose a prompt-guided
                    multimodal interaction framework. </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
  <p>
    <center>
      &copy; Sule Bai | Last updated: Mar. 2025
    </center>
  </p>
</body>

</html>